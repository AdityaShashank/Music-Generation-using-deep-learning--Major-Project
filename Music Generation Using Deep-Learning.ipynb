{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re \n",
    "import pandas \n",
    "print(pandas.__file__)\n",
    "import numpy as np \n",
    "from glob import glob\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.utils import np_utils\n",
    "import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = glob('Jazz/*.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jazz\\\\2ndMovementOfSinisterFootwear.mid', 'Jazz\\\\55Dive.mid', 'Jazz\\\\5To10.mid', 'Jazz\\\\634-5789.mid', 'Jazz\\\\914.mid', 'Jazz\\\\ABC.mid', 'Jazz\\\\ACertainSmile.mid']\n"
     ]
    }
   ],
   "source": [
    "songs = songs[:7]\n",
    "print(songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    notes = []\n",
    "    for file in songs:\n",
    "        # converting .mid file to stream object\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = []\n",
    "        try:\n",
    "            # Given a single stream, partition into a part for each unique instrument\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "        except:\n",
    "            pass\n",
    "        if parts: # if parts has instrument parts \n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else:\n",
    "            notes_to_parse = midi.flat.notes\n",
    "    \n",
    "        for element in notes_to_parse: \n",
    "            if isinstance(element, note.Note):\n",
    "                # if element is a note, extract pitch\n",
    "                notes.append(str(element.pitch))\n",
    "            elif(isinstance(element, chord.Chord)):\n",
    "                # if element is a chord, append the normal form of the \n",
    "                # chord (a list of integers) to the list of notes. \n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab): \n",
    "    sequence_length = 100\n",
    "\n",
    "    # Extract the unique pitches in the list of notes.\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i: i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "    \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    # reshape the input into a format comatible with LSTM layers \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    # one hot encode the output vectors\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, Flatten\n",
    "def create_network(network_in, n_vocab): \n",
    "    \"\"\"Create the model architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=network_in.shape[1:], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "  \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "def train(model, network_input, network_output, epochs): \n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \"\"\"\n",
    "    # Create checkpoint to save the best model weights.\n",
    "    filepath = 'weights.best.music3.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True)\n",
    "    \n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=32, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network():\n",
    "    \"\"\"\n",
    "    Get notes\n",
    "    Generates input and output sequences\n",
    "    Creates a model \n",
    "    Trains the model for the given epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = 20\n",
    "    with open('data/notes', 'rb') as filepath:notes = pickle.load(filepath)\n",
    "\n",
    "    print('Notes processed')\n",
    "    \n",
    "    n_vocab = len(set(notes))\n",
    "    print('Vocab generated')\n",
    "    \n",
    "    network_in, network_out = prepare_sequences(notes, n_vocab)\n",
    "    print('Input and Output processed')\n",
    "    \n",
    "    model = create_network(network_in, n_vocab)\n",
    "    print('Model created')\n",
    "    \n",
    "    print('Training in progress')\n",
    "    train(model, network_in, network_out, epochs)\n",
    "    print('Training completed')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes processed\n",
      "Vocab generated\n",
      "Input and Output processed\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 128)          66560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 138)               35466     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 138)               0         \n",
      "=================================================================\n",
      "Total params: 3,510,666\n",
      "Trainable params: 3,510,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model created\n",
      "Training in progress\n",
      "WARNING:tensorflow:From C:\\Users\\adity\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "1184/1184 [==============================] - ETA: 1:05 - loss: 4.936 - ETA: 36s - loss: 4.887 - ETA: 26s - loss: 4.79 - ETA: 21s - loss: 4.84 - ETA: 18s - loss: 4.86 - ETA: 16s - loss: 4.86 - ETA: 14s - loss: 4.86 - ETA: 13s - loss: 4.84 - ETA: 11s - loss: 4.81 - ETA: 11s - loss: 4.78 - ETA: 10s - loss: 4.79 - ETA: 9s - loss: 4.7995 - ETA: 9s - loss: 4.769 - ETA: 8s - loss: 4.742 - ETA: 8s - loss: 4.722 - ETA: 7s - loss: 4.711 - ETA: 7s - loss: 4.699 - ETA: 6s - loss: 4.693 - ETA: 6s - loss: 4.682 - ETA: 5s - loss: 4.664 - ETA: 5s - loss: 4.654 - ETA: 5s - loss: 4.647 - ETA: 4s - loss: 4.636 - ETA: 4s - loss: 4.640 - ETA: 3s - loss: 4.625 - ETA: 3s - loss: 4.613 - ETA: 3s - loss: 4.608 - ETA: 2s - loss: 4.605 - ETA: 2s - loss: 4.595 - ETA: 2s - loss: 4.588 - ETA: 1s - loss: 4.577 - ETA: 1s - loss: 4.577 - ETA: 1s - loss: 4.569 - ETA: 0s - loss: 4.553 - ETA: 0s - loss: 4.553 - ETA: 0s - loss: 4.557 - 11s 10ms/step - loss: 4.5521\n",
      "Epoch 2/20\n",
      "1184/1184 [==============================] - ETA: 10s - loss: 4.06 - ETA: 9s - loss: 4.2945 - ETA: 8s - loss: 4.368 - ETA: 8s - loss: 4.323 - ETA: 8s - loss: 4.309 - ETA: 7s - loss: 4.357 - ETA: 7s - loss: 4.400 - ETA: 7s - loss: 4.379 - ETA: 6s - loss: 4.366 - ETA: 6s - loss: 4.376 - ETA: 6s - loss: 4.379 - ETA: 6s - loss: 4.375 - ETA: 5s - loss: 4.374 - ETA: 5s - loss: 4.381 - ETA: 5s - loss: 4.365 - ETA: 5s - loss: 4.372 - ETA: 4s - loss: 4.364 - ETA: 4s - loss: 4.370 - ETA: 4s - loss: 4.368 - ETA: 4s - loss: 4.356 - ETA: 3s - loss: 4.359 - ETA: 3s - loss: 4.351 - ETA: 3s - loss: 4.353 - ETA: 3s - loss: 4.349 - ETA: 2s - loss: 4.355 - ETA: 2s - loss: 4.371 - ETA: 2s - loss: 4.378 - ETA: 2s - loss: 4.372 - ETA: 2s - loss: 4.373 - ETA: 1s - loss: 4.378 - ETA: 1s - loss: 4.378 - ETA: 1s - loss: 4.377 - ETA: 1s - loss: 4.371 - ETA: 0s - loss: 4.373 - ETA: 0s - loss: 4.374 - ETA: 0s - loss: 4.381 - 9s 8ms/step - loss: 4.3736\n",
      "Epoch 3/20\n",
      "1184/1184 [==============================] - ETA: 10s - loss: 4.21 - ETA: 9s - loss: 4.2271 - ETA: 9s - loss: 4.233 - ETA: 9s - loss: 4.201 - ETA: 8s - loss: 4.282 - ETA: 8s - loss: 4.277 - ETA: 8s - loss: 4.295 - ETA: 7s - loss: 4.296 - ETA: 7s - loss: 4.324 - ETA: 7s - loss: 4.317 - ETA: 7s - loss: 4.332 - ETA: 6s - loss: 4.317 - ETA: 6s - loss: 4.306 - ETA: 6s - loss: 4.324 - ETA: 6s - loss: 4.314 - ETA: 5s - loss: 4.297 - ETA: 5s - loss: 4.311 - ETA: 5s - loss: 4.306 - ETA: 4s - loss: 4.299 - ETA: 4s - loss: 4.280 - ETA: 4s - loss: 4.283 - ETA: 3s - loss: 4.289 - ETA: 3s - loss: 4.289 - ETA: 3s - loss: 4.295 - ETA: 3s - loss: 4.301 - ETA: 2s - loss: 4.302 - ETA: 2s - loss: 4.298 - ETA: 2s - loss: 4.294 - ETA: 2s - loss: 4.309 - ETA: 1s - loss: 4.311 - ETA: 1s - loss: 4.305 - ETA: 1s - loss: 4.319 - ETA: 1s - loss: 4.320 - ETA: 0s - loss: 4.317 - ETA: 0s - loss: 4.318 - ETA: 0s - loss: 4.319 - 10s 9ms/step - loss: 4.3239\n",
      "Epoch 4/20\n",
      "1184/1184 [==============================] - ETA: 12s - loss: 4.46 - ETA: 12s - loss: 4.31 - ETA: 11s - loss: 4.34 - ETA: 10s - loss: 4.40 - ETA: 10s - loss: 4.31 - ETA: 9s - loss: 4.2995 - ETA: 8s - loss: 4.279 - ETA: 8s - loss: 4.283 - ETA: 7s - loss: 4.281 - ETA: 7s - loss: 4.314 - ETA: 7s - loss: 4.356 - ETA: 6s - loss: 4.349 - ETA: 6s - loss: 4.342 - ETA: 6s - loss: 4.348 - ETA: 5s - loss: 4.350 - ETA: 5s - loss: 4.334 - ETA: 5s - loss: 4.352 - ETA: 5s - loss: 4.349 - ETA: 4s - loss: 4.333 - ETA: 4s - loss: 4.319 - ETA: 4s - loss: 4.330 - ETA: 4s - loss: 4.325 - ETA: 3s - loss: 4.329 - ETA: 3s - loss: 4.333 - ETA: 3s - loss: 4.329 - ETA: 2s - loss: 4.330 - ETA: 2s - loss: 4.327 - ETA: 2s - loss: 4.324 - ETA: 2s - loss: 4.328 - ETA: 1s - loss: 4.331 - ETA: 1s - loss: 4.320 - ETA: 1s - loss: 4.321 - ETA: 1s - loss: 4.329 - ETA: 0s - loss: 4.329 - ETA: 0s - loss: 4.323 - ETA: 0s - loss: 4.326 - 10s 8ms/step - loss: 4.3283\n",
      "Epoch 5/20\n",
      "1184/1184 [==============================] - ETA: 8s - loss: 4.389 - ETA: 8s - loss: 4.346 - ETA: 8s - loss: 4.353 - ETA: 9s - loss: 4.321 - ETA: 9s - loss: 4.284 - ETA: 9s - loss: 4.280 - ETA: 8s - loss: 4.311 - ETA: 8s - loss: 4.302 - ETA: 7s - loss: 4.300 - ETA: 7s - loss: 4.289 - ETA: 7s - loss: 4.292 - ETA: 6s - loss: 4.275 - ETA: 6s - loss: 4.255 - ETA: 6s - loss: 4.250 - ETA: 5s - loss: 4.257 - ETA: 5s - loss: 4.293 - ETA: 5s - loss: 4.299 - ETA: 4s - loss: 4.304 - ETA: 4s - loss: 4.306 - ETA: 4s - loss: 4.312 - ETA: 4s - loss: 4.306 - ETA: 3s - loss: 4.311 - ETA: 3s - loss: 4.302 - ETA: 3s - loss: 4.294 - ETA: 3s - loss: 4.298 - ETA: 2s - loss: 4.289 - ETA: 2s - loss: 4.292 - ETA: 2s - loss: 4.290 - ETA: 2s - loss: 4.280 - ETA: 1s - loss: 4.280 - ETA: 1s - loss: 4.278 - ETA: 1s - loss: 4.275 - ETA: 1s - loss: 4.276 - ETA: 0s - loss: 4.273 - ETA: 0s - loss: 4.285 - ETA: 0s - loss: 4.287 - 10s 8ms/step - loss: 4.2869\n",
      "Epoch 6/20\n",
      "1184/1184 [==============================] - ETA: 10s - loss: 4.22 - ETA: 9s - loss: 4.1750 - ETA: 9s - loss: 4.167 - ETA: 8s - loss: 4.196 - ETA: 8s - loss: 4.166 - ETA: 8s - loss: 4.164 - ETA: 8s - loss: 4.217 - ETA: 8s - loss: 4.236 - ETA: 7s - loss: 4.272 - ETA: 7s - loss: 4.265 - ETA: 7s - loss: 4.279 - ETA: 7s - loss: 4.271 - ETA: 6s - loss: 4.259 - ETA: 6s - loss: 4.254 - ETA: 6s - loss: 4.252 - ETA: 5s - loss: 4.234 - ETA: 5s - loss: 4.221 - ETA: 5s - loss: 4.228 - ETA: 5s - loss: 4.215 - ETA: 4s - loss: 4.194 - ETA: 4s - loss: 4.201 - ETA: 4s - loss: 4.194 - ETA: 3s - loss: 4.196 - ETA: 3s - loss: 4.203 - ETA: 3s - loss: 4.199 - ETA: 3s - loss: 4.197 - ETA: 2s - loss: 4.187 - ETA: 2s - loss: 4.189 - ETA: 2s - loss: 4.184 - ETA: 1s - loss: 4.181 - ETA: 1s - loss: 4.186 - ETA: 1s - loss: 4.179 - ETA: 1s - loss: 4.176 - ETA: 0s - loss: 4.181 - ETA: 0s - loss: 4.182 - ETA: 0s - loss: 4.188 - 10s 8ms/step - loss: 4.1928\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1184 [==============================] - ETA: 11s - loss: 3.95 - ETA: 9s - loss: 3.9938 - ETA: 9s - loss: 3.947 - ETA: 9s - loss: 3.937 - ETA: 8s - loss: 3.949 - ETA: 8s - loss: 3.916 - ETA: 8s - loss: 3.927 - ETA: 7s - loss: 3.919 - ETA: 7s - loss: 3.924 - ETA: 7s - loss: 3.946 - ETA: 7s - loss: 3.944 - ETA: 6s - loss: 3.964 - ETA: 6s - loss: 3.983 - ETA: 6s - loss: 3.988 - ETA: 6s - loss: 3.967 - ETA: 5s - loss: 3.972 - ETA: 5s - loss: 3.972 - ETA: 5s - loss: 3.979 - ETA: 4s - loss: 3.993 - ETA: 4s - loss: 3.998 - ETA: 4s - loss: 3.991 - ETA: 4s - loss: 3.986 - ETA: 3s - loss: 3.993 - ETA: 3s - loss: 3.989 - ETA: 3s - loss: 3.984 - ETA: 2s - loss: 3.980 - ETA: 2s - loss: 3.991 - ETA: 2s - loss: 4.009 - ETA: 2s - loss: 4.011 - ETA: 1s - loss: 4.028 - ETA: 1s - loss: 4.040 - ETA: 1s - loss: 4.046 - ETA: 1s - loss: 4.045 - ETA: 0s - loss: 4.045 - ETA: 0s - loss: 4.047 - ETA: 0s - loss: 4.047 - 10s 9ms/step - loss: 4.0444\n",
      "Epoch 8/20\n",
      "1184/1184 [==============================] - ETA: 11s - loss: 4.12 - ETA: 10s - loss: 4.03 - ETA: 10s - loss: 4.00 - ETA: 9s - loss: 3.9535 - ETA: 9s - loss: 3.892 - ETA: 8s - loss: 3.898 - ETA: 8s - loss: 3.921 - ETA: 8s - loss: 3.899 - ETA: 7s - loss: 3.854 - ETA: 7s - loss: 3.853 - ETA: 7s - loss: 3.868 - ETA: 6s - loss: 3.859 - ETA: 6s - loss: 3.857 - ETA: 6s - loss: 3.849 - ETA: 5s - loss: 3.888 - ETA: 5s - loss: 3.896 - ETA: 5s - loss: 3.895 - ETA: 5s - loss: 3.886 - ETA: 4s - loss: 3.885 - ETA: 4s - loss: 3.899 - ETA: 4s - loss: 3.893 - ETA: 4s - loss: 3.908 - ETA: 3s - loss: 3.903 - ETA: 3s - loss: 3.909 - ETA: 3s - loss: 3.903 - ETA: 3s - loss: 3.902 - ETA: 2s - loss: 3.906 - ETA: 2s - loss: 3.915 - ETA: 2s - loss: 3.913 - ETA: 1s - loss: 3.912 - ETA: 1s - loss: 3.905 - ETA: 1s - loss: 3.901 - ETA: 1s - loss: 3.912 - ETA: 0s - loss: 3.914 - ETA: 0s - loss: 3.918 - ETA: 0s - loss: 3.921 - 10s 8ms/step - loss: 3.9144\n",
      "Epoch 9/20\n",
      "1184/1184 [==============================] - ETA: 10s - loss: 3.93 - ETA: 9s - loss: 3.7755 - ETA: 9s - loss: 3.648 - ETA: 8s - loss: 3.668 - ETA: 8s - loss: 3.685 - ETA: 8s - loss: 3.646 - ETA: 7s - loss: 3.666 - ETA: 7s - loss: 3.715 - ETA: 7s - loss: 3.761 - ETA: 6s - loss: 3.761 - ETA: 6s - loss: 3.766 - ETA: 6s - loss: 3.777 - ETA: 6s - loss: 3.785 - ETA: 5s - loss: 3.789 - ETA: 5s - loss: 3.782 - ETA: 5s - loss: 3.790 - ETA: 5s - loss: 3.792 - ETA: 4s - loss: 3.804 - ETA: 4s - loss: 3.806 - ETA: 4s - loss: 3.809 - ETA: 4s - loss: 3.801 - ETA: 3s - loss: 3.785 - ETA: 3s - loss: 3.785 - ETA: 3s - loss: 3.778 - ETA: 3s - loss: 3.779 - ETA: 2s - loss: 3.778 - ETA: 2s - loss: 3.788 - ETA: 2s - loss: 3.786 - ETA: 2s - loss: 3.794 - ETA: 1s - loss: 3.798 - ETA: 1s - loss: 3.802 - ETA: 1s - loss: 3.808 - ETA: 1s - loss: 3.814 - ETA: 0s - loss: 3.820 - ETA: 0s - loss: 3.832 - ETA: 0s - loss: 3.840 - 9s 8ms/step - loss: 3.8391\n",
      "Epoch 10/20\n",
      "1184/1184 [==============================] - ETA: 10s - loss: 3.65 - ETA: 9s - loss: 3.6461 - ETA: 9s - loss: 3.697 - ETA: 8s - loss: 3.651 - ETA: 8s - loss: 3.679 - ETA: 8s - loss: 3.667 - ETA: 7s - loss: 3.699 - ETA: 7s - loss: 3.723 - ETA: 7s - loss: 3.714 - ETA: 6s - loss: 3.701 - ETA: 6s - loss: 3.700 - ETA: 6s - loss: 3.712 - ETA: 5s - loss: 3.733 - ETA: 5s - loss: 3.728 - ETA: 5s - loss: 3.709 - ETA: 5s - loss: 3.709 - ETA: 5s - loss: 3.701 - ETA: 4s - loss: 3.700 - ETA: 4s - loss: 3.689 - ETA: 4s - loss: 3.695 - ETA: 4s - loss: 3.712 - ETA: 3s - loss: 3.710 - ETA: 3s - loss: 3.707 - ETA: 3s - loss: 3.701 - ETA: 3s - loss: 3.704 - ETA: 2s - loss: 3.699 - ETA: 2s - loss: 3.709 - ETA: 2s - loss: 3.706 - ETA: 2s - loss: 3.714 - ETA: 1s - loss: 3.719 - ETA: 1s - loss: 3.716 - ETA: 1s - loss: 3.720 - ETA: 1s - loss: 3.724 - ETA: 0s - loss: 3.726 - ETA: 0s - loss: 3.728 - ETA: 0s - loss: 3.733 - 10s 8ms/step - loss: 3.7388\n",
      "Epoch 11/20\n",
      "1184/1184 [==============================] - ETA: 12s - loss: 3.90 - ETA: 10s - loss: 3.75 - ETA: 10s - loss: 3.61 - ETA: 9s - loss: 3.5737 - ETA: 9s - loss: 3.629 - ETA: 9s - loss: 3.638 - ETA: 8s - loss: 3.625 - ETA: 8s - loss: 3.604 - ETA: 7s - loss: 3.588 - ETA: 7s - loss: 3.544 - ETA: 7s - loss: 3.540 - ETA: 6s - loss: 3.547 - ETA: 6s - loss: 3.538 - ETA: 6s - loss: 3.532 - ETA: 5s - loss: 3.556 - ETA: 5s - loss: 3.534 - ETA: 5s - loss: 3.542 - ETA: 4s - loss: 3.533 - ETA: 4s - loss: 3.573 - ETA: 4s - loss: 3.577 - ETA: 4s - loss: 3.558 - ETA: 3s - loss: 3.569 - ETA: 3s - loss: 3.563 - ETA: 3s - loss: 3.571 - ETA: 3s - loss: 3.572 - ETA: 2s - loss: 3.584 - ETA: 2s - loss: 3.590 - ETA: 2s - loss: 3.592 - ETA: 2s - loss: 3.597 - ETA: 1s - loss: 3.587 - ETA: 1s - loss: 3.599 - ETA: 1s - loss: 3.602 - ETA: 1s - loss: 3.601 - ETA: 0s - loss: 3.603 - ETA: 0s - loss: 3.600 - ETA: 0s - loss: 3.606 - 9s 8ms/step - loss: 3.6103\n",
      "Epoch 12/20\n",
      "1184/1184 [==============================] - ETA: 11s - loss: 3.04 - ETA: 9s - loss: 3.1613 - ETA: 9s - loss: 3.322 - ETA: 8s - loss: 3.412 - ETA: 8s - loss: 3.403 - ETA: 8s - loss: 3.407 - ETA: 7s - loss: 3.389 - ETA: 7s - loss: 3.356 - ETA: 7s - loss: 3.356 - ETA: 6s - loss: 3.361 - ETA: 6s - loss: 3.356 - ETA: 6s - loss: 3.360 - ETA: 6s - loss: 3.358 - ETA: 5s - loss: 3.353 - ETA: 5s - loss: 3.361 - ETA: 5s - loss: 3.397 - ETA: 5s - loss: 3.391 - ETA: 4s - loss: 3.421 - ETA: 4s - loss: 3.450 - ETA: 4s - loss: 3.452 - ETA: 4s - loss: 3.460 - ETA: 3s - loss: 3.466 - ETA: 3s - loss: 3.462 - ETA: 3s - loss: 3.472 - ETA: 3s - loss: 3.479 - ETA: 2s - loss: 3.497 - ETA: 2s - loss: 3.507 - ETA: 2s - loss: 3.513 - ETA: 2s - loss: 3.522 - ETA: 1s - loss: 3.532 - ETA: 1s - loss: 3.537 - ETA: 1s - loss: 3.536 - ETA: 1s - loss: 3.544 - ETA: 0s - loss: 3.537 - ETA: 0s - loss: 3.537 - ETA: 0s - loss: 3.539 - 9s 8ms/step - loss: 3.5471\n",
      "Epoch 13/20\n",
      "1184/1184 [==============================] - ETA: 9s - loss: 3.098 - ETA: 8s - loss: 3.279 - ETA: 8s - loss: 3.221 - ETA: 8s - loss: 3.275 - ETA: 7s - loss: 3.269 - ETA: 7s - loss: 3.264 - ETA: 7s - loss: 3.304 - ETA: 7s - loss: 3.268 - ETA: 6s - loss: 3.240 - ETA: 6s - loss: 3.214 - ETA: 6s - loss: 3.224 - ETA: 6s - loss: 3.248 - ETA: 5s - loss: 3.265 - ETA: 5s - loss: 3.268 - ETA: 5s - loss: 3.299 - ETA: 5s - loss: 3.307 - ETA: 4s - loss: 3.307 - ETA: 4s - loss: 3.308 - ETA: 4s - loss: 3.312 - ETA: 4s - loss: 3.300 - ETA: 3s - loss: 3.305 - ETA: 3s - loss: 3.321 - ETA: 3s - loss: 3.317 - ETA: 3s - loss: 3.331 - ETA: 2s - loss: 3.317 - ETA: 2s - loss: 3.329 - ETA: 2s - loss: 3.346 - ETA: 2s - loss: 3.347 - ETA: 1s - loss: 3.345 - ETA: 1s - loss: 3.358 - ETA: 1s - loss: 3.360 - ETA: 1s - loss: 3.372 - ETA: 0s - loss: 3.367 - ETA: 0s - loss: 3.375 - ETA: 0s - loss: 3.377 - ETA: 0s - loss: 3.388 - 9s 8ms/step - loss: 3.3952\n",
      "Epoch 14/20\n",
      "1184/1184 [==============================] - ETA: 11s - loss: 3.28 - ETA: 9s - loss: 3.2965 - ETA: 8s - loss: 3.230 - ETA: 7s - loss: 3.253 - ETA: 7s - loss: 3.283 - ETA: 7s - loss: 3.293 - ETA: 6s - loss: 3.251 - ETA: 6s - loss: 3.250 - ETA: 6s - loss: 3.276 - ETA: 6s - loss: 3.259 - ETA: 6s - loss: 3.251 - ETA: 5s - loss: 3.225 - ETA: 5s - loss: 3.228 - ETA: 5s - loss: 3.222 - ETA: 5s - loss: 3.223 - ETA: 5s - loss: 3.226 - ETA: 4s - loss: 3.217 - ETA: 4s - loss: 3.204 - ETA: 4s - loss: 3.206 - ETA: 4s - loss: 3.234 - ETA: 3s - loss: 3.253 - ETA: 3s - loss: 3.244 - ETA: 3s - loss: 3.248 - ETA: 3s - loss: 3.243 - ETA: 2s - loss: 3.245 - ETA: 2s - loss: 3.242 - ETA: 2s - loss: 3.235 - ETA: 2s - loss: 3.242 - ETA: 1s - loss: 3.243 - ETA: 1s - loss: 3.235 - ETA: 1s - loss: 3.258 - ETA: 1s - loss: 3.259 - ETA: 0s - loss: 3.258 - ETA: 0s - loss: 3.266 - ETA: 0s - loss: 3.262 - ETA: 0s - loss: 3.267 - 9s 7ms/step - loss: 3.2735\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1184 [==============================] - ETA: 11s - loss: 2.80 - ETA: 9s - loss: 2.9382 - ETA: 8s - loss: 2.821 - ETA: 7s - loss: 2.863 - ETA: 7s - loss: 2.894 - ETA: 7s - loss: 2.899 - ETA: 7s - loss: 2.937 - ETA: 6s - loss: 2.961 - ETA: 6s - loss: 2.962 - ETA: 6s - loss: 3.004 - ETA: 6s - loss: 3.006 - ETA: 5s - loss: 3.003 - ETA: 5s - loss: 3.026 - ETA: 5s - loss: 3.046 - ETA: 5s - loss: 3.044 - ETA: 4s - loss: 3.034 - ETA: 4s - loss: 3.063 - ETA: 4s - loss: 3.079 - ETA: 4s - loss: 3.077 - ETA: 3s - loss: 3.083 - ETA: 3s - loss: 3.076 - ETA: 3s - loss: 3.075 - ETA: 3s - loss: 3.054 - ETA: 2s - loss: 3.066 - ETA: 2s - loss: 3.080 - ETA: 2s - loss: 3.089 - ETA: 2s - loss: 3.083 - ETA: 2s - loss: 3.083 - ETA: 1s - loss: 3.086 - ETA: 1s - loss: 3.087 - ETA: 1s - loss: 3.094 - ETA: 1s - loss: 3.110 - ETA: 0s - loss: 3.105 - ETA: 0s - loss: 3.109 - ETA: 0s - loss: 3.117 - ETA: 0s - loss: 3.119 - 8s 7ms/step - loss: 3.1153\n",
      "Epoch 16/20\n",
      "1184/1184 [==============================] - ETA: 10s - loss: 2.84 - ETA: 8s - loss: 2.7731 - ETA: 7s - loss: 2.752 - ETA: 7s - loss: 2.684 - ETA: 7s - loss: 2.698 - ETA: 7s - loss: 2.709 - ETA: 7s - loss: 2.692 - ETA: 6s - loss: 2.745 - ETA: 6s - loss: 2.766 - ETA: 6s - loss: 2.804 - ETA: 6s - loss: 2.814 - ETA: 5s - loss: 2.841 - ETA: 5s - loss: 2.820 - ETA: 5s - loss: 2.824 - ETA: 5s - loss: 2.832 - ETA: 4s - loss: 2.806 - ETA: 4s - loss: 2.814 - ETA: 4s - loss: 2.822 - ETA: 4s - loss: 2.832 - ETA: 3s - loss: 2.821 - ETA: 3s - loss: 2.826 - ETA: 3s - loss: 2.833 - ETA: 3s - loss: 2.832 - ETA: 2s - loss: 2.839 - ETA: 2s - loss: 2.856 - ETA: 2s - loss: 2.846 - ETA: 2s - loss: 2.861 - ETA: 2s - loss: 2.857 - ETA: 1s - loss: 2.867 - ETA: 1s - loss: 2.865 - ETA: 1s - loss: 2.871 - ETA: 1s - loss: 2.867 - ETA: 0s - loss: 2.873 - ETA: 0s - loss: 2.885 - ETA: 0s - loss: 2.881 - ETA: 0s - loss: 2.890 - 8s 7ms/step - loss: 2.8922\n",
      "Epoch 17/20\n",
      "1184/1184 [==============================] - ETA: 11s - loss: 2.39 - ETA: 10s - loss: 2.69 - ETA: 9s - loss: 2.6695 - ETA: 8s - loss: 2.564 - ETA: 7s - loss: 2.543 - ETA: 7s - loss: 2.530 - ETA: 7s - loss: 2.495 - ETA: 6s - loss: 2.492 - ETA: 6s - loss: 2.516 - ETA: 6s - loss: 2.494 - ETA: 6s - loss: 2.518 - ETA: 5s - loss: 2.529 - ETA: 5s - loss: 2.540 - ETA: 5s - loss: 2.563 - ETA: 5s - loss: 2.590 - ETA: 4s - loss: 2.603 - ETA: 4s - loss: 2.612 - ETA: 4s - loss: 2.635 - ETA: 4s - loss: 2.637 - ETA: 4s - loss: 2.641 - ETA: 3s - loss: 2.648 - ETA: 3s - loss: 2.655 - ETA: 3s - loss: 2.673 - ETA: 3s - loss: 2.680 - ETA: 2s - loss: 2.701 - ETA: 2s - loss: 2.703 - ETA: 2s - loss: 2.703 - ETA: 2s - loss: 2.687 - ETA: 1s - loss: 2.697 - ETA: 1s - loss: 2.707 - ETA: 1s - loss: 2.716 - ETA: 1s - loss: 2.714 - ETA: 0s - loss: 2.725 - ETA: 0s - loss: 2.713 - ETA: 0s - loss: 2.712 - ETA: 0s - loss: 2.718 - 9s 7ms/step - loss: 2.7174\n",
      "Epoch 18/20\n",
      "1184/1184 [==============================] - ETA: 12s - loss: 2.19 - ETA: 10s - loss: 2.11 - ETA: 9s - loss: 2.1521 - ETA: 8s - loss: 2.271 - ETA: 7s - loss: 2.244 - ETA: 7s - loss: 2.195 - ETA: 7s - loss: 2.200 - ETA: 7s - loss: 2.229 - ETA: 6s - loss: 2.267 - ETA: 6s - loss: 2.289 - ETA: 6s - loss: 2.296 - ETA: 6s - loss: 2.279 - ETA: 5s - loss: 2.290 - ETA: 5s - loss: 2.281 - ETA: 5s - loss: 2.269 - ETA: 4s - loss: 2.272 - ETA: 4s - loss: 2.254 - ETA: 4s - loss: 2.263 - ETA: 4s - loss: 2.289 - ETA: 3s - loss: 2.294 - ETA: 3s - loss: 2.298 - ETA: 3s - loss: 2.307 - ETA: 3s - loss: 2.326 - ETA: 2s - loss: 2.350 - ETA: 2s - loss: 2.352 - ETA: 2s - loss: 2.347 - ETA: 2s - loss: 2.349 - ETA: 2s - loss: 2.360 - ETA: 1s - loss: 2.369 - ETA: 1s - loss: 2.393 - ETA: 1s - loss: 2.408 - ETA: 1s - loss: 2.427 - ETA: 0s - loss: 2.425 - ETA: 0s - loss: 2.439 - ETA: 0s - loss: 2.454 - ETA: 0s - loss: 2.462 - 8s 7ms/step - loss: 2.4665\n",
      "Epoch 19/20\n",
      "1184/1184 [==============================] - ETA: 11s - loss: 2.27 - ETA: 9s - loss: 2.2193 - ETA: 9s - loss: 2.242 - ETA: 8s - loss: 2.230 - ETA: 8s - loss: 2.290 - ETA: 7s - loss: 2.268 - ETA: 7s - loss: 2.218 - ETA: 7s - loss: 2.187 - ETA: 6s - loss: 2.192 - ETA: 6s - loss: 2.168 - ETA: 6s - loss: 2.199 - ETA: 6s - loss: 2.185 - ETA: 5s - loss: 2.191 - ETA: 5s - loss: 2.210 - ETA: 5s - loss: 2.189 - ETA: 4s - loss: 2.202 - ETA: 4s - loss: 2.205 - ETA: 4s - loss: 2.198 - ETA: 4s - loss: 2.223 - ETA: 3s - loss: 2.216 - ETA: 3s - loss: 2.198 - ETA: 3s - loss: 2.208 - ETA: 3s - loss: 2.217 - ETA: 2s - loss: 2.218 - ETA: 2s - loss: 2.224 - ETA: 2s - loss: 2.226 - ETA: 2s - loss: 2.219 - ETA: 1s - loss: 2.231 - ETA: 1s - loss: 2.247 - ETA: 1s - loss: 2.252 - ETA: 1s - loss: 2.238 - ETA: 1s - loss: 2.246 - ETA: 0s - loss: 2.263 - ETA: 0s - loss: 2.269 - ETA: 0s - loss: 2.267 - ETA: 0s - loss: 2.280 - 8s 7ms/step - loss: 2.2831\n",
      "Epoch 20/20\n",
      "1184/1184 [==============================] - ETA: 10s - loss: 2.42 - ETA: 8s - loss: 2.2102 - ETA: 8s - loss: 2.171 - ETA: 8s - loss: 2.158 - ETA: 7s - loss: 2.098 - ETA: 7s - loss: 2.100 - ETA: 7s - loss: 2.147 - ETA: 6s - loss: 2.099 - ETA: 6s - loss: 2.099 - ETA: 6s - loss: 2.112 - ETA: 6s - loss: 2.135 - ETA: 5s - loss: 2.126 - ETA: 5s - loss: 2.128 - ETA: 5s - loss: 2.134 - ETA: 5s - loss: 2.115 - ETA: 4s - loss: 2.109 - ETA: 4s - loss: 2.123 - ETA: 4s - loss: 2.119 - ETA: 4s - loss: 2.107 - ETA: 3s - loss: 2.099 - ETA: 3s - loss: 2.078 - ETA: 3s - loss: 2.078 - ETA: 3s - loss: 2.073 - ETA: 3s - loss: 2.090 - ETA: 2s - loss: 2.099 - ETA: 2s - loss: 2.121 - ETA: 2s - loss: 2.124 - ETA: 2s - loss: 2.117 - ETA: 1s - loss: 2.125 - ETA: 1s - loss: 2.114 - ETA: 1s - loss: 2.112 - ETA: 1s - loss: 2.121 - ETA: 0s - loss: 2.129 - ETA: 0s - loss: 2.128 - ETA: 0s - loss: 2.130 - ETA: 0s - loss: 2.130 - 9s 7ms/step - loss: 2.1374\n",
      "Training completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1b77196a940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train the model \n",
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    #load the notes used to train the model\n",
    "    with open('data/notes', 'rb') as filepath:\n",
    "        notes = pickle.load(filepath)\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # Get all pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    print('Initiating music generation process.......')\n",
    "    \n",
    "    network_input = get_inputSequences(notes, pitchnames, n_vocab)\n",
    "    normalized_input = network_input / float(n_vocab)\n",
    "    model = create_network(normalized_input, n_vocab)\n",
    "    print('Loading Model weights.....')\n",
    "    model.load_weights('weights.best.music3.hdf5')\n",
    "    print('Model Loaded')\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputSequences(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 100\n",
    "    network_input = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    \n",
    "    network_input = np.reshape(network_input, (len(network_input), 100, 1))\n",
    "    \n",
    "    return (network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Pick a random integer\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    pattern = list(network_input[start])\n",
    "    prediction_output = []\n",
    "    \n",
    "    print('Generating notes........')\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        \n",
    "        # Predicted output is the argmax(P(h|D))\n",
    "        index = np.argmax(prediction)\n",
    "        # Mapping the predicted interger back to the corresponding note\n",
    "        result = int_to_note[index]\n",
    "        # Storing the predicted output\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        # Next input to the model\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    print('Notes Generated...')\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    \n",
    "    print('Saving Output file as midi....')\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output6.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating music generation process.......\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100, 128)          66560     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 138)               35466     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 138)               0         \n",
      "=================================================================\n",
      "Total params: 3,510,666\n",
      "Trainable params: 3,510,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loading Model weights.....\n",
      "Model Loaded\n",
      "Generating notes........\n",
      "Notes Generated...\n",
      "Saving Output file as midi....\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'test_output6.mid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6b9ec19b5a0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#### Generate a new jazz music\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-1f246ccd9305>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Loaded'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprediction_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_notes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpitchnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mcreate_midi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-5e33e4e81130>\u001b[0m in \u001b[0;36mcreate_midi\u001b[1;34m(prediction_output)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saving Output file as midi....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mmidi_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'midi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test_output6.mid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\music21\\base.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, fmt, fp, **keywords)\u001b[0m\n\u001b[0;32m   2034\u001b[0m         \u001b[0mscClass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindSubConverterForFormat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregularizedConverterFormat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2035\u001b[0m         \u001b[0mformatWriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2036\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mformatWriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizedConverterFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubformats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2038\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\music21\\converter\\subConverters.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, obj, fmt, fp, subformats, **keywords)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmidiTranslate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmusic21ObjectToMidiFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# write binary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[0mmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\music21\\midi\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, filename, attrib)\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattrib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mMidiException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cannot read or write unless in binary mode, not:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mopenFileLike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'test_output6.mid'"
     ]
    }
   ],
   "source": [
    "#### Generate a new jazz music \n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Play the Jazz music\n",
    "play.play_midi('test_output6.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
